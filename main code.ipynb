{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d74f77cc-021c-4bad-aa26-b799fd596884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\amart\\anaconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\amart\\anaconda3\\lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\amart\\anaconda3\\lib\\site-packages (5.2.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\amart\\anaconda3\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\amart\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\amart\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\amart\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from transformers) (1.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\amart\\anaconda3\\lib\\site-packages (from transformers) (0.24.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.27.0)\n",
      "Requirement already satisfied: shellingham in c:\\users\\amart\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\amart\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: typer>=0.24.0 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from typer-slim->transformers) (0.24.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\amart\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\amart\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\amart\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\amart\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\amart\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.14.0)\n",
      "Requirement already satisfied: click>=8.2.1 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from typer>=0.24.0->typer-slim->transformers) (8.3.1)\n",
      "Requirement already satisfied: rich>=12.3.0 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from typer>=0.24.0->typer-slim->transformers) (13.7.1)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from typer>=0.24.0->typer-slim->transformers) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\amart\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision transformers pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7cf81af-65c1-43df-be61-6d98c2733f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a1277bc-8cc8-46e4-b49a-1b2655459b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df7d0fff-4b20-4291-b161-868cf595cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ccf295a-d006-4960-a668-d216185d242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "027e7388-d5da-4435-89ef-bd2dff4a0866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf3ffad33764a188a0a855ff60cd736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mCLIPModel LOAD REPORT\u001b[0m from: openai/clip-vit-base-patch32\n",
      "Key                                  | Status     |  | \n",
      "-------------------------------------+------------+--+-\n",
      "vision_model.embeddings.position_ids | UNEXPECTED |  | \n",
      "text_model.embeddings.position_ids   | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "The image processor of type `CLIPImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n"
     ]
    }
   ],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d35e8a2-9df0-4e7f-8635-868b7b9d5684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tags(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    candidate_labels = [\n",
    "        \"person\", \"dog\", \"cat\", \"car\", \"street\", \"tree\",\n",
    "        \"building\", \"food\", \"computer\", \"phone\",\n",
    "        \"beach\", \"mountain\", \"indoor\", \"outdoor\",\n",
    "        \"animal\", \"man\", \"woman\", \"child\"\n",
    "    ]\n",
    "\n",
    "    inputs = clip_processor(\n",
    "        text=candidate_labels,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        probs = logits_per_image.softmax(dim=1)\n",
    "\n",
    "    probs = probs[0]\n",
    "\n",
    "    tags = []\n",
    "    for i, prob in enumerate(probs):\n",
    "        if prob.item() > 0.20:   # confidence threshold\n",
    "            tags.append(candidate_labels[i])\n",
    "\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40bd84a9-6dd1-4f49-ac9e-14064197e976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eaef66532094d3bb683c8d00a7c9d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "llm_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2401f28b-1853-412b-8373-d5891dce64f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    outputs = llm_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=120,\n",
    "        temperature=0.9,\n",
    "        top_p=0.95,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81c9ec01-3f2d-4ed9-8634-fdc2ce4d7819",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "def build_prompt(tags, question):\n",
    "    tag_text = \", \".join(tags)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an AI assistant that answers questions about an image.\n",
    "\n",
    "The image contains: {tag_text}.\n",
    "\n",
    "If the user asks to describe the image, summarize what is visible using the detected objects.\n",
    "\n",
    "If the user asks about something not in the detected objects, politely say it is not detected.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in a clear full sentence:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1886f220-411b-4290-b09a-c6732deaa8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected image tags: ['cat']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask about the image (type 'exit' to quit):  What animal is in the picture?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: cat\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask about the image (type 'exit' to quit):  Is this indoor or outdoor?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: indoor\n"
     ]
    }
   ],
   "source": [
    "image_path = \"test_images/cat1.jpg\"  # change if needed\n",
    "\n",
    "tags = generate_tags(image_path)\n",
    "print(\"Detected image tags:\", tags)\n",
    "\n",
    "while True:\n",
    "    question = input(\"Ask about the image (type 'exit' to quit): \")\n",
    "\n",
    "    if question.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    prompt = build_prompt(tags, question)\n",
    "    response = generate_response(prompt)\n",
    "\n",
    "    print(\"Assistant:\", response)\n",
    "\n",
    "    conversation_history.append({\n",
    "        \"user\": question,\n",
    "        \"assistant\": response\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41722ccc-9a2b-455c-a8d4-382cc14acaab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
